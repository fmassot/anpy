#!/usr/bin/env python3

import json
import sys
import os
import bs4
import io
import glob
import tqdm
from urllib.parse import urljoin, urlparse
import zipfile
import time

from pathlib import Path

import attr
import click
import requests
import slugify

from anpy.dossier import Dossier
from anpy.dossier_like_senapy import parse as parse_dossier_like_senapy
from anpy.question import parse_question
from anpy.amendement import Amendement, AmendementSearchService
from anpy.scrutin import Scrutin
from anpy.utils import json_dumps

from lawfactory_utils.urls import download, enable_requests_cache


sys.path.append(str(Path(__file__).absolute().parents[1]))


def is_valid_dosleg_resp(resp):
    if resp.status_code < 300:
        if "vous prie d'accepter toutes ses excuses pour le" in resp.text:
            print('[INVALID RESPONSE]', resp.url)
        else:
            return True
    else:
        print('[INVALID STATUS CODE]', resp.status_code, resp.url)
    return False



def dossier_id(url):
    scheme, netloc, path, params, query, fragment = urlparse(url)
    return slugify.slugify(path.replace('dossiers/', '') \
        .replace('.asp', ''))


def _log(*args):
    print(*args, file=sys.stderr)


@click.group()
def cli():
    pass


@cli.command()
@click.argument('id-dossier')
@click.option('--id-examen')
@click.option('--limit', default=100)
def show_amendements_order(id_dossier, id_examen, limit):
    results = AmendementSearchService().get_order(
        idDossierLegislatif=id_dossier, idExamen=id_examen, rows=limit)
    print(u'Nombre d\'amendements   : {}'.format(len(results)))
    print(u'Ordre des ammendements : {}'.format((','.join(results))))


@cli.command()
@click.option('--start-date')
@click.option('--end-date')
@click.option('--numero')
@click.option('--rows', default=100)
def show_amendements_summary(start_date, end_date, numero, rows):
    iterator = AmendementSearchService().iterator(rows=rows,
                                                  dateDebut=start_date,
                                                  dateFin=end_date,
                                                  numAmend=numero)
    for result in iterator:
        print(json.dumps(attr.asdict(result), indent=4, sort_keys=True,
                         ensure_ascii=False))


@cli.command()
@click.argument('url')
def show_amendement(url):
    print(u'Amendement : {}'.format(url))
    print(json.dumps(Amendement.download_and_build(url).__dict__,
                     indent=4, sort_keys=True, ensure_ascii=False))


@cli.command()
@click.argument('url')
def show_question(url):
    question_html = download(url + '/vue/xml').content
    parsed_data = parse_question(url, question_html)
    print(json.dumps(parsed_data, indent=4, sort_keys=True,
                     ensure_ascii=False))


@cli.command()
@click.argument('url')
def show_dossier(url):
    dossier = Dossier.download_and_build(url)
    print(json_dumps(dossier.to_dict(), indent=4, sort_keys=True,
                     ensure_ascii=False))


@cli.command()
@click.argument('url')
def show_dossier_like_senapy(url):
    if os.path.exists(url):
        try:
            html = open(url).read()
        except UnicodeDecodeError:
            html = open(url, encoding='iso-8859-1').read()
        url = html.split('-- URL=')[-1].split('-->')[0].strip()
    else:
        resp = download(url)
        resp.encoding = 'Windows-1252'
        html = resp.text

    print(json_dumps(parse_dossier_like_senapy(html, url), indent=4, sort_keys=True,
                     ensure_ascii=False))


@cli.command()
@click.argument('url')
def show_scrutin(url):
    scrutin = Scrutin.download_and_build(url)
    print(json_dumps(scrutin.to_dict(), indent=4, sort_keys=True,
                     ensure_ascii=False))


@cli.command()
def doslegs_urls():
    _log('## Finding doslegs urls in assemblee-nationale.fr...(since 13th legislature)')
    urls_website = set()
    INDEX_URLS = [
        'http://www.assemblee-nationale.fr/15/documents/index-dossier.asp',
        'http://www.assemblee-nationale.fr/14/documents/index-dossier.asp',
        'http://www.assemblee-nationale.fr/13/documents/index-dossier.asp',
        # 'http://www.assemblee-nationale.fr/12/documents/index-dossier.asp',
        # 'http://www.assemblee-nationale.fr/11/documents/index-dossier.asp',

        'http://www.assemblee-nationale.fr/15/documents/index-conventions.asp',
        'http://www.assemblee-nationale.fr/14/documents/index-conventions.asp',
        'http://www.assemblee-nationale.fr/13/documents/index-conventions.asp',

        # 'http://www2.assemblee-nationale.fr/documents/liste/(ajax)/1/(offset)/0/(limit)/100000/(legis)/15/(type)/propositions-loi/',
        # 'http://www2.assemblee-nationale.fr/documents/liste/(ajax)/1/(offset)/0/(limit)/100000/(legis)/14/(type)/propositions-loi/',
        'http://www.assemblee-nationale.fr/13/documents/index-proposition.asp',

        # 'http://www2.assemblee-nationale.fr/documents/liste/(ajax)/1/(offset)/0/(limit)/100000/(legis)/15/(type)/projets-loi/',
        # 'http://www2.assemblee-nationale.fr/documents/liste/(ajax)/1/(offset)/0/(limit)/100000/(legis)/14/(type)/projets-loi/',
        'http://www.assemblee-nationale.fr/13/documents/index-projets.asp',

        'http://www2.assemblee-nationale.fr/documents/liste/(ajax)/1/(offset)/0/(limit)/100000/(legis)/15/(type)/depots/',
        'http://www2.assemblee-nationale.fr/documents/liste/(ajax)/1/(offset)/0/(limit)/100000/(legis)/14/(type)/depots/',
        'http://www.assemblee-nationale.fr/13/documents/index-depots.asp',
    ]

    for index_url in INDEX_URLS:
        _log('     * scanning', index_url, '...')

        for link in bs4.BeautifulSoup(download(index_url).text, 'lxml').select('a'):
            url = urljoin('http://www.assemblee-nationale.fr', link.attrs.get('href', ''))

            if '/dossiers/' in url:
                urls_website.add(url)

    _log('  => found', len(urls_website), 'doslegs')
    _log()


    _log('## Finding doslegs urls in Open Data...')
    urls_opendata = set()
    doslegs_resp = download('http://data.assemblee-nationale.fr/static/openData/repository/LOI/dossiers_legislatifs/Dossiers_Legislatifs_XIV.json.zip')
    doslegs_zip = zipfile.ZipFile(io.BytesIO(doslegs_resp.content))
    DATA = json.loads(doslegs_zip.open('Dossiers_Legislatifs_XIV.json').read().decode('utf-8'))

    for dossier in DATA['export']['dossiersLegislatifs']['dossier']:
        titreChemin = dossier['dossierParlementaire']['titreDossier']['titreChemin']
        if not titreChemin:
            _log('  - INVALID titreChemin attribute -', titreChemin)
            continue

        url = 'http://www.assemblee-nationale.fr/{}/dossiers/{}.asp'.format(
                dossier['dossierParlementaire']['legislature'], titreChemin)

        urls_opendata.add(url)

    _log('  => found', len(urls_opendata), 'doslegs')

    for url in urls_website | urls_opendata:
        print(url)


@cli.command()
@click.argument('output_dir')
@click.option('--overwrite', is_flag=True)
@click.option('--disable-cache', is_flag=True)
def parse_many_doslegs(output_dir, overwrite, disable_cache):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    if not disable_cache:
        enable_requests_cache()

    for url in sys.stdin:
        url = url.strip()
        filepath = os.path.join(output_dir, dossier_id(url))
        if not overwrite and os.path.exists(filepath):
            continue

        print(' -- ', url)
        resp = download(url)
        if is_valid_dosleg_resp(resp):
            results = parse_dossier_like_senapy(resp.text, url, verbose=False)

            if results:
                for i, dos in enumerate(results):
                    full_filepath = filepath
                    if i != 0:
                        full_filepath += '_%d' % i
                    open(full_filepath, 'w').write(json.dumps(dos, indent=4, sort_keys=True, ensure_ascii=False))
            else:
                open(filepath, 'w').write(json.dumps(None, indent=4, sort_keys=True, ensure_ascii=False))


if __name__ == '__main__':
    cli()
